{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acca9637-fe84-4a1a-ba2a-c5831a5006d9",
   "metadata": {},
   "source": [
    "# darepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6f686-9412-497c-9694-a553c36cc865",
   "metadata": {},
   "source": [
    "This package is designed to aid in the efficient analysis of large simulations, such as cosmological (hydrodynamical) simulations of large-scale structure.\n",
    "\n",
    "It uses the [dask](https://dask.org/) library to perform computations, which has two key advantages:\n",
    "* (i) very large datasets which cannot normally fit into memory can be analyzed, and\n",
    "* (ii) calculations can be automatically distributed onto parallel 'workers', across one or more (MPI) nodes, to speed them up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bafbcf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [0] Select a simulation and snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4b3ec1-a0cc-4596-a0d0-b3bced75d22a",
   "metadata": {},
   "source": [
    "The first step is to chose an existing snapshot of a simulation. To start, we will intentionally select the $z=0$ output of TNG50-4, which is the lowest resolution version of TNG50. This means that the size of data in the snapshot is small and easy to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d5180c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Specified path does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17785/3159397336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdarepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marepo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArepoSnapshot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../sims.TNG/TNG50-4/output/snapdir_099/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msnap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArepoSnapshot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/repos/darepo/darepo/interfaces/arepo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, catalog)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatalog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/darepo/darepo/interface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBaseSnapshot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mdefaultattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/darepo/darepo/interface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Specified path does not exist.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Specified path does not exist."
     ]
    }
   ],
   "source": [
    "from darepo.interfaces.arepo import ArepoSnapshot\n",
    "path = \"../../sims.TNG/TNG50-4/output/snapdir_099/\"\n",
    "snap = ArepoSnapshot(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ddc3f7",
   "metadata": {},
   "source": [
    "# [1] Get familiar with a snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fc0a0",
   "metadata": {},
   "source": [
    "## Header metadata\n",
    "The snapshot contains a dictionary for the AREPO simulation header, config and parameters in its namespace. We can inspect the header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe2006",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap.header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4870aa",
   "metadata": {},
   "source": [
    "Note that only keys that are the same across all snapshot file chunks are is. \n",
    "Entries which are different for each file, such as `NumPart_ThisFile`, are stacked along the first axis, so that we also have access to this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap.header['NumPart_ThisFile']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c068f07",
   "metadata": {},
   "source": [
    "## Particle/cell data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba185d31",
   "metadata": {},
   "source": [
    "Within our `snap` object, `snap.data` contains references to all the particle/cell data in this snapshot.\n",
    "\n",
    "If the snapshot is split across multiple file chunks on disk (as is the case for most large cosmological simulations), then these are virtually \"combined\".\n",
    "\n",
    "As a result, there is a single array per field in `snap.data`. Note that these are **not** normal numpy arrays, but are instead **dask arrays**, which we will return to later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0aaaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap.data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ce158",
   "metadata": {},
   "source": [
    "Let's list all fields available for the respective particle species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fea0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,val in snap.data.items():\n",
    "    print(\"Species:\", key)\n",
    "    print(val.keys(), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a6cc6",
   "metadata": {},
   "source": [
    "Note that none of these datasets have actually been loaded yet! Instead, what we have available is a convenient way to access the data via dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28d0cd",
   "metadata": {},
   "source": [
    "# [2] Analyzing snapshot data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cffb98",
   "metadata": {},
   "source": [
    "In order to perform a given analysis on some available snapshot data, we would normally first explicitly load the required data from disk, and then run some calculations on this data (in memory).\n",
    "\n",
    "Instead, with dask, our fields are loaded automatically as well as \"lazily\" -- only when actually required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe8a42-898d-473b-ac02-7055659bd9c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Computing a simple statistic on (all) particles\n",
    "\n",
    "The fields in our snapshot object behave similar to actual numpy arrays. \n",
    "\n",
    "As a first simple example, let's calculate the total mass of gas cells in the entire simulation. Just as in numpy we can write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf9fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = snap.data[\"PartType0\"][\"Masses\"]\n",
    "task = mass.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd3f47",
   "metadata": {},
   "source": [
    "Note that all objects remain 'virtual': they are not calculated or loaded from disk, but are merely the required instructions, encoded into tasks. In a notebook we can inspect these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0905903",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81ab05",
   "metadata": {},
   "source": [
    "We can request a calculation of the actual operation(s) by applying the `.compute()` method to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61532442-15d8-4186-8e13-1f04c8c06389",
   "metadata": {},
   "source": [
    "## Creating a visualization: projecting onto a 2D image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44bd67",
   "metadata": {},
   "source": [
    "As an example of calculating something more complicated than just `sum()`, let's do the usual \"poor man's projection\" via a 2D histogram.\n",
    "\n",
    "To do so, we use [da.histogram2d()](https://docs.dask.org/en/latest/array.html) of dask, which is analogous to [numpy.histogram2d()](https://numpy.org/doc/stable/reference/generated/numpy.histogram2d.html), except that it operates on a dask array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4da5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "coords = snap.data[\"PartType0\"][\"Coordinates\"]\n",
    "x = coords[:,0]\n",
    "y = coords[:,1]\n",
    "\n",
    "nbins = 128\n",
    "bins1d = np.linspace(0,snap.header[\"BoxSize\"],nbins+1)\n",
    "\n",
    "result = da.histogram2d(x,y,bins=[bins1d,bins1d])\n",
    "im2d = result[0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe3f8f-6e44-4999-bde3-655821e737ee",
   "metadata": {},
   "source": [
    "The resulting `im2d` is just a two-dimensional array which we can display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10d128-f744-4fd0-8999-cf49e2b31179",
   "metadata": {},
   "outputs": [],
   "source": [
    "im2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.imshow(im2d,norm=LogNorm())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ae331",
   "metadata": {},
   "source": [
    "# [3] Scaling up: handling a large simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76b964e",
   "metadata": {},
   "source": [
    "Until now, we have applied our framework to a very small simulation. \n",
    "\n",
    "However, what if we are working with a very large simulation (like TNG50-1, with $8^3 = 512$ times more particles/cells)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac4b1c-d45a-4150-beb7-a47ea8059bc3",
   "metadata": {},
   "source": [
    "## Starting simple: computing in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122b753-9c0a-4447-85ed-65405c1cb315",
   "metadata": {},
   "source": [
    "First, we can still run the same calculation as above, and it will \"just work\" (hopefully).\n",
    "\n",
    "This is because Dask has many versions of common algorithms and functions which work on \"blocks\" or \"chunks\" of the data, which split up the large array into smaller arrays. Work is needed on each chunk, after which the final answer is assembled.\n",
    "\n",
    "Importantly, in our case above, even if the `mass` array above does not fit into memory, the `mass.sum().compute()` will chunk the operation up in a way that the task can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be63e28-0545-4f52-bc82-42a75da29291",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_large = ArepoSnapshot(path.replace('TNG50-4','TNG50-2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd9640-196a-40eb-a2ed-9ddb3d2818d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_large.header[\"NumPart_Total\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27247b3-1cef-4a58-8710-2368b5b9354a",
   "metadata": {},
   "source": [
    "Before we start, let's enable a progress indicator from dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f90a0d-1bb1-4926-b8c7-54512bd0aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e34ea36-5f92-4b16-b2bc-38d8d8c271a4",
   "metadata": {},
   "source": [
    "And then we can request the actual computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c5c47-cb72-41d6-b41a-f307508beb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_large.data[\"PartType0\"][\"Masses\"].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47063b-519b-465c-bc8a-0ed878071944",
   "metadata": {},
   "source": [
    "While the result is eventually computed, it is a bit slow, primarily because the actual reading of the data off disk is the limiting factor, and this is happening in serial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68150ad2-c46b-46b2-9a49-8f07182afb46",
   "metadata": {},
   "source": [
    "## More advanced: computing in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cdaa67-0cff-4c7e-991f-8117f28d3e8b",
   "metadata": {},
   "source": [
    "Rather than sequentially calculating large tasks, we can also run the computation in parallel. \n",
    "\n",
    "To do so different advanced dask schedulers are available. Here, we use the most straight forward [distributed scheduler](https://docs.dask.org/en/latest/how-to/deploy-dask/single-distributed.html).\n",
    "\n",
    "Usually, we would start a scheduler and then connect new workers (e.g. running on multiple compute/backend nodes of a HPC cluster). After, tasks (either interactively or scripted) can leverage the power of these connected resources.\n",
    "\n",
    "For this example, we will use the same \"distributed\" scheduler/API, but keep things simple by using just the one (local) node we are currently running on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a047f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=4,threads_per_worker=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbaf174-e652-4f17-a877-3f8d9c569ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464efbf",
   "metadata": {},
   "source": [
    "We can now perform the same operations, but it is performed in a distributed manner, in parallel.\n",
    "\n",
    "One significant advantage is that (even when using only a single node) individual workers will load just the subsets of data they need to work on, meaing that I/O operations become parallel.\n",
    "\n",
    "Note: after creating a `Client()`, all calls to `.compute()` will automatically use this new set of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2bd97-7885-4953-b785-ddc25963a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = snap_large.data[\"PartType0\"][\"Masses\"].sum()\n",
    "task.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30be9a6-d1f2-4b4a-b25d-b84516eeb169",
   "metadata": {},
   "source": [
    "We can also view the progress of this task as it executes. For the distributed scheduler, a status dashboard exists (as a webpage).\n",
    "\n",
    "You can find it by clicking on the \"Dashboard\" link above. If running this notebook server remotely, e.g. on a login node of a HPC cluster, you may have to change the '127.0.0.1' part of the address to be the same machine name/IP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32426f20",
   "metadata": {},
   "source": [
    "# [4] AREPO/TNG sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1407a0",
   "metadata": {},
   "source": [
    "## [4.1] Units\n",
    "For Arepo simulations, units via 'astropy.units' are available by default. These are hardcoded from the public AREPO and Illustris TNG documentation. Arrays will hold the units upon evalation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de085d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darepo.interfaces.arepo import ArepoSnapshotWithUnits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f80043",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap = ArepoSnapshotWithUnits(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap.data[\"PartType0\"][\"Masses\"].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fedcf",
   "metadata": {},
   "source": [
    "# [4.2] Catalog information\n",
    "We can additionally pass the group and subhalo catalogs. The catalogs will be loaded into snap.data as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03302a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darepo.interfaces.arepo import ArepoSnapshot\n",
    "path = \"/data/cbyrohl/TNGdata/TNG50-4/output/snapdir_099/\"\n",
    "snap = ArepoSnapshot(path,catalog=path.replace(\"snapdir\",\"groups\"))\n",
    "snap.data[\"Group\"][\"GroupMass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ead85",
   "metadata": {},
   "source": [
    "Additionally, we can determine the halo/subhalo an arbitrary particle belongs to. Let's get all group and subhalo IDs for all black holes! A '-1' indicates the lack of an associated group/subhalo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e9e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Group IDs for chosen particles:\", snap.data[\"PartType5\"][\"GroupID\"].compute())\n",
    "print(\"Subhalo IDs for chosen particles:\", snap.data[\"PartType5\"][\"SubhaloID\"].compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c95795",
   "metadata": {},
   "source": [
    "## [4.3] Selecting halos\n",
    "We can select all particle data for a given halo ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = snap.return_data(haloID=42) # the result contains all fields for all particle types for a given halo\n",
    "data[\"PartType0\"][\"Density\"] # density for all gas particles in halo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d7e12",
   "metadata": {},
   "source": [
    "# [4.4] Apply same operation to all halos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de3d3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No caching directory specified. Initial file read will remain slow.\n",
      "WARNING:root:No caching directory specified. Initial file read will remain slow.\n"
     ]
    }
   ],
   "source": [
    "from darepo.interfaces.arepo import ArepoSnapshot\n",
    "path = \"/home/cbyrohl/TNGdata/TNG50-4/output/snapdir_030/\"\n",
    "snap = ArepoSnapshot(path,catalog=path.replace(\"snapdir\",\"groups\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35c567b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4119409e+02, 9.6530640e+01, 9.0133446e+01, ..., 6.0309753e-02,\n",
       "       4.1851357e-02, 3.0552845e-02], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_mass(mass, fieldnames=[\"Masses\"], parttype=\"PartType0\"):\n",
    "    \"\"\"Return all gas mass in halo\"\"\"\n",
    "    return mass.sum()\n",
    "\n",
    "totgasmasses = snap.map_halo_operation(calculate_mass).compute()\n",
    "totgasmasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c08327cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.372824 ,  4.7390265,  7.1822767, ...,  0.       ,  0.       ,\n",
       "        0.       ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_mass(mass, sfrs, fieldnames=[\"Masses\",\"StarFormationRate\"], parttype=\"PartType0\"):\n",
    "    \"\"\"Return all gas mass that is star-forming.\"\"\"\n",
    "    return mass[sfrs>0].sum()\n",
    "\n",
    "totgasmasses_sf = snap.map_halo_operation(calculate_mass).compute()\n",
    "totgasmasses_sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a7286-5a16-4ae8-a058-8bb76eb1a130",
   "metadata": {},
   "source": [
    "# [5] Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a53b37-f6a3-46e0-b9f4-baad9c26f7bf",
   "metadata": {},
   "source": [
    "(beyond simple statistics/build-in numpy methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d779c4ae-38f1-468c-b98d-fcb11af97741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ed860a7-5a5e-44a2-b7b2-317dd8eb5bd4",
   "metadata": {},
   "source": [
    "# [6] Derived Snapshot Fields\n",
    "We can easily add recipes for new fields. For this, we need to define a new function\n",
    "```\n",
    "def field(arrs, **kwargs):\n",
    "    newarr = ...arrs['key']...\n",
    "    return newarr\n",
    "```\n",
    "\n",
    "where 'arrs' is a dictionary holding the keys available for the given data type (e.g. gas) and can be used to deduce new fields. The new function is to be decorated by \n",
    "\n",
    "```\n",
    "@snap.register_field(parttype, name=\"fieldname\")\n",
    "def field(arrs, **kwargs):\n",
    "    ...\n",
    "```\n",
    "\n",
    "if no name is given, the function name will be used for it. See an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135dff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darepo.interface import ArepoSnapshot\n",
    "path = \"/data/cbyrohl/TNGdata/TNG50-4/output/snapdir_099/\"\n",
    "snap = ArepoSnapshot(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f6b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "import astropy.constants as const\n",
    "\n",
    "@snap.register_field(\"gas\")\n",
    "def Temperature(arrs, **kwargs):\n",
    "    xh = 0.76\n",
    "    eabu = (arrs['ElectronAbundance'],1.0)\n",
    "    eint = (arrs['InternalEnergy'],(u.km/u.s)**2)\n",
    "    mu = 4.0 / (1.0 + 3 * xh + 4 * xh * eabu[0])\n",
    "    mu_unit = 1.0 /eabu[1] * const.m_p\n",
    "    gamma = 5.0 / 3.0\n",
    "    temp = ((gamma - 1.0) * eint[0] * mu)\n",
    "    temp_unit = (mu_unit * eint[1] /const.k_B).to(u.K)\n",
    "    return (temp*temp_unit).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = snap.data[\"PartType0\"][\"Temperature\"]\n",
    "temp.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c15cef-79a8-4505-a5f2-fdc4daa3de73",
   "metadata": {},
   "source": [
    "# [7] Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d82d28-6a45-4070-9870-442314866299",
   "metadata": {},
   "source": [
    "(what the caching warning means, how to use some caching)\n",
    "\n",
    "(some links to docs of relevance etc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
